{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "from glob import glob\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import efficientnet.tfkeras as efn\n",
    "import scipy as sp\n",
    "from functools import partial\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now that we have the data preprocessed, we are ready to begin training our model. In order to consume the tfrecords we created previously we will use the powerful Tensorflow Dataset API. \n",
    "\n",
    "## Table of Contents\n",
    "1. [Creating the dataset](#Creating-the-dataset)\n",
    "2. [Model and Kappa optimizer](#Model-and-Kappa-optimizer)\n",
    "3. [Training](#Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "PREPROCESSING = 'subtract_median'\n",
    "AUGMENT_P = .25\n",
    "DROPOUT_P = .2\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER = BATCH_SIZE * 10\n",
    "SEED = 10011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_speed(ds):\n",
    "    begin = time.time()\n",
    "    i = 0\n",
    "    for x in ds:\n",
    "        i+=1\n",
    "    for x in ds:\n",
    "        pass\n",
    "    end = time.time()\n",
    "    elapsed = (end-begin)/2\n",
    "    result = '{:.3f} seconds for {} images or batches, {:.3f} ms/image or batch'.format(elapsed, i, 1000*elapsed/i)\n",
    "    print(result)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "for ds in DATASETS:\n",
    "    files[ds] = glob(os.path.join(DATA_DIR, ds, 'preproc', PREPROCESSING, f'{IMAGE_SIZE}', '*.tfrecords'))\n",
    "    files[ds].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files = [files['aptos'][1]]\n",
    "train_files = [files['aptos'][0]]\n",
    "for ds in DATASETS:\n",
    "    if ds != 'aptos':\n",
    "        train_files+=files[ds]\n",
    "random.shuffle(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.TFRecordDataset(train_files, num_parallel_reads=None)\n",
    "val_ds = tf.data.TFRecordDataset(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = test_speed(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(parse_function, AUTOTUNE)\n",
    "val_ds = val_ds.map(parse_function, AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = test_speed(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kdonbekci/anaconda3/envs/aptos/lib/python3.6/site-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_ds.shuffle(SHUFFLE_BUFFER, SEED)\n",
    "val_ds = val_ds.shuffle(SHUFFLE_BUFFER, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x: {'diagnosis': tf.cast(x['diagnosis'], dtype=tf.float32), 'id': x['id'], \n",
    "                                   'img': tf.image.convert_image_dtype(tf.io.decode_jpeg(x['img']), dtype=tf.float32)}, AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x: {'diagnosis': tf.cast(x['diagnosis'], dtype=tf.float32), 'id': x['id'], \n",
    "                               'img': tf.image.convert_image_dtype(tf.io.decode_jpeg(x['img']), dtype=tf.float32)}, AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def rotate(x):\n",
    "    return tf.image.rot90(x, tf.random.uniform(shape=[], minval=1, maxval=3, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def flip(x):\n",
    "    return tf.image.random_flip_left_right(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def color(x):\n",
    "    x = tf.image.random_hue(x, 0.08)\n",
    "    x = tf.image.random_saturation(x, 0.6, 1.6)\n",
    "    x = tf.image.random_brightness(x, 0.05)\n",
    "    x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = [rotate, flip, color]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def augment(x):\n",
    "    for f in augmentations:\n",
    "        x = tf.cond(tf.random.uniform([], 0, 1) < AUGMENT_P, lambda: f(x), lambda: x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x: ({'img': augment(x['img'])}, {'diagnosis': x['diagnosis']}), AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x: ({'img': x['img'], 'id': x['id']}, {'diagnosis': x['diagnosis']}), AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.689 seconds for 26 images or batches, 26.511 ms/image or batch\n"
     ]
    }
   ],
   "source": [
    "_ = test_speed(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Kappa optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder:\n",
    "    def __init__(self):\n",
    "        self.coefficients = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = metrics.cohen_kappa_score(y, X_p, weights='quadratic')\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coefficients = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')['x']\n",
    "    \n",
    "    def predict(self, X, coef):\n",
    "        return np.digitize(X, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, load=False, name='', image_size=IMAGE_SIZE, preprocessing=PREPROCESSING, **kwargs):\n",
    "        self.optimizer = OptimizedRounder()\n",
    "        if not load:\n",
    "            self.log = {}\n",
    "            self.log['description'] = []\n",
    "            self.log['trained_for'] = 0\n",
    "            self.log['name'] = name\n",
    "            self.log['preprocessing'] = preprocessing\n",
    "            self.log['augmentations'] = [f.__name__ for f in augmentations]\n",
    "            self.log['birthday'] = get_time('%m-%d_%H-%M-%S')\n",
    "            self.log['image_size'] = image_size\n",
    "            self.log['augment_p'] = AUGMENT_P\n",
    "            self.log['dropout_p'] = DROPOUT_P\n",
    "            self.log['batch_size'] = BATCH_SIZE\n",
    "            self.log['family_dir'] = os.path.join(MODELS_DIR,'{}'.format(name))\n",
    "            if not os.path.exists(self.log['family_dir']):\n",
    "                os.mkdir(self.log['family_dir'])\n",
    "            self.log['model_dir'] = os.path.join(self.log['family_dir'], self.log['birthday'])\n",
    "            if not os.path.exists(self.log['model_dir']):\n",
    "                os.mkdir(self.log['model_dir'])        \n",
    "            self.log['tb_dir'] = os.path.join(self.log['model_dir'], 'tb')\n",
    "            if not os.path.exists(self.log['tb_dir']):\n",
    "                os.mkdir(self.log['tb_dir'])\n",
    "            self.log['checkpoints_dir'] = os.path.join(self.log['model_dir'], 'model_checkpoints')\n",
    "            if not os.path.exists(self.log['checkpoints_dir']):\n",
    "                os.mkdir(self.log['checkpoints_dir'])            \n",
    "            self.log['history'] = []\n",
    "    \n",
    "    def update_history(self, history):\n",
    "        self.log['history'].append(history)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        history = self.model.fit(*args, **kwargs)\n",
    "        self.log['history'].append(history)\n",
    "        self.log['trained_for'] += kwargs['steps_per_epoch'] * kwargs['epochs']\n",
    "        return history\n",
    "    \n",
    "    def predict(self, ds, with_truth):\n",
    "        if with_truth:\n",
    "            truth = []\n",
    "            preds = []\n",
    "            for x, y in val_ds:\n",
    "                truth += list(y['diagnosis'].numpy())\n",
    "                preds += list(self(x['img'])['diagnosis'].numpy().flatten())\n",
    "            return np.array(truth), np.array(preds)\n",
    "        else:\n",
    "            return self.model.predict(ds).flatten()\n",
    "    \n",
    "    def fit_optimizer(self, truth, preds):\n",
    "        self.optimizer.fit(preds, truth)\n",
    "    \n",
    "    def optimize_predictions(self, preds):\n",
    "        return self.optimizer.predict(preds, self.optimizer.coefficients)\n",
    "    \n",
    "    def calculate_kappa(self, truth, preds):\n",
    "        return metrics.cohen_kappa_score(truth, preds, weights='quadratic')\n",
    "\n",
    "    def save(self, description=None):\n",
    "        if description:\n",
    "            self.log['description'].append(description)\n",
    "        i = len(glob(os.path.join(self.log['checkpoints_dir'], '*/')))\n",
    "        path = os.path.join(self.log['checkpoints_dir'], 'checkpoint_{:02d}'.format(i))\n",
    "        os.mkdir(path)\n",
    "        self.log['optimizer_coefficients'] = self.optimizer.coefficients\n",
    "        self.log['optimizer'] = self.model.optimizer.get_config()\n",
    "        self.model.save(os.path.join(path, 'model.h5'), overwrite=False, include_optimizer=False)\n",
    "        save_pickle(self.log, os.path.join(path, 'log.pkl'))\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "     \n",
    "    @staticmethod\n",
    "    def load(family, birthday, checkpoint, custom_objects=None):\n",
    "        model = Model(load=True)\n",
    "        if MACHINE == 'kaggle':\n",
    "            path = os.path.join(SRC_DIR, 'aptos-{}-{}'.format(family, ''.join([c for c in birthday if c != '-']).replace('_', '-')), \n",
    "                                f'{birthday}', 'model_checkpoints', 'checkpoint_{:02d}'.format(checkpoint))\n",
    "        elif MACHINE == 'local':\n",
    "            path = os.path.join(MODELS_DIR, family, birthday, 'model_checkpoints', 'checkpoint_{:02d}'.format(checkpoint))\n",
    "        model.log = load_pickle(os.path.join(path, 'log.pkl'))\n",
    "        model.model = tf.keras.models.load_model(os.path.join(path, 'model.h5'), compile=False, custom_objects=custom_objects)\n",
    "        optimizer = tf.keras.optimizers.get(model.log['optimizer']['name']).from_config(model.log['optimizer']) \n",
    "        model.model.compile(optimizer=optimizer, loss=fix_customs(model.log['loss']), \n",
    "                            metrics=fix_customs(model.log['metrics']))\n",
    "        model.optimizer.coefficients = model.log['optimizer_coefficients']\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        name = 'baseline'\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.model = self.build_model()\n",
    "        self.log['description'].append('using efficientnetb0 base as pretrained and training the top layer with regression')\n",
    "        \n",
    "    def build_model(self):\n",
    "        optimizer = 'adam'\n",
    "        loss = {'diagnosis': 'mean_squared_error'}\n",
    "        metrics = {'diagnosis': ['r2']}\n",
    "        self.log['loss'] = loss\n",
    "        self.log['metrics'] = metrics\n",
    "\n",
    "        img_input = tf.keras.layers.Input(shape= (IMAGE_SIZE, IMAGE_SIZE, 3), name='img')\n",
    "\n",
    "        B0_base = efn.EfficientNetB0(include_top=False, weights='imagenet', \n",
    "                                     input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), pooling='avg')\n",
    "\n",
    "        B0_base.trainable = False\n",
    "\n",
    "        x = B0_base(img_input)\n",
    "\n",
    "        x = tf.keras.layers.Dropout(rate=DROPOUT_P, name='dropout_1')(x)\n",
    "\n",
    "        out = tf.keras.layers.Dense(1, activation=None, name='diagnosis')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs={'img': img_input}, outputs={'diagnosis': out}, name='Baseline') \n",
    "\n",
    "\n",
    "\n",
    "        model.compile(optimizer='adam', loss=fix_customs(loss), metrics=fix_customs(metrics))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = tf.keras.callbacks.TensorBoard(log_dir = model.log['tb_dir'], write_graph=False, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 300 steps, validate for 26 steps\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 60s 199ms/step - loss: 0.8759 - r2: 0.0997 - val_loss: 0.8055 - val_r2: 0.5235\n",
      "Epoch 2/20\n",
      "227/300 [=====================>........] - ETA: 11s - loss: 0.7920 - r2: 0.1264"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_ds, steps_per_epoch=300, epochs=20, validation_data=val_ds, validation_steps=26,\n",
    "                           callbacks = [tb], shuffle=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "loss = {'diagnosis': 'mean_squared_error'}\n",
    "metrics = {'diagnosis': ['r2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.model.weights\n",
    "model.model.layers[1].trainable = True\n",
    "model.model.layers[-1].trainable = False\n",
    "model.model.compile(optimizer='adam', loss=fix_customs(loss), metrics=fix_customs(metrics))\n",
    "model.model.weights = weights\n",
    "history=model.fit(train_ds, steps_per_epoch=300, epochs=10, validation_data=val_ds, validation_steps=26,\n",
    "                           callbacks = [tb], shuffle=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.model.weights\n",
    "model.model.layers[1].trainable = False\n",
    "model.model.layers[-1].trainable = True\n",
    "model.model.compile(optimizer='adam', loss=fix_customs(loss), metrics=fix_customs(metrics))\n",
    "model.model.weights = weights\n",
    "history=model.fit(train_ds, steps_per_epoch=300, epochs=20, validation_data=val_ds, validation_steps=26,\n",
    "                           callbacks = [tb], shuffle=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.model.weights\n",
    "model.model.layers[1].trainable = True\n",
    "model.model.layers[-1].trainable = False\n",
    "model.model.compile(optimizer='adam', loss=fix_customs(loss), metrics=fix_customs(metrics))\n",
    "model.model.weights = weights\n",
    "history=model.fit(train_ds, steps_per_epoch=300, epochs=10, validation_data=val_ds, validation_steps=26,\n",
    "                           callbacks = [tb], shuffle=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.model.weights\n",
    "model.model.layers[1].trainable = False\n",
    "model.model.layers[-1].trainable = True\n",
    "model.model.compile(optimizer='adam', loss=fix_customs(loss), metrics=fix_customs(metrics))\n",
    "model.model.weights = weights\n",
    "history=model.fit(train_ds, steps_per_epoch=300, epochs=20, validation_data=val_ds, validation_steps=26,\n",
    "                           callbacks = [tb], shuffle=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(description='back and forth training, top-base-top-base-top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth, preds = model.predict(val_ds, with_truth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_optimizer(truth, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_preds =  model.optimize_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = model.calculate_kappa(truth, optimized_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(description='kappa {:.4f} after training back and forth'.format(kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo poweroff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BEST KAPPA .83\n",
    "family='baseline', birthday='08-29_18-05-53', checkpoint=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.load(family='baseline', birthday='08-29_18-05-53', checkpoint=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth, preds = model.predict(val_ds, with_truth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_optimizer(truth, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_preds =  model.optimize_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = model.calculate_kappa(truth, optimized_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8279213458450164\n"
     ]
    }
   ],
   "source": [
    "print(kappa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aptos",
   "language": "python",
   "name": "aptos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
