{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import concurrent.futures\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import measure, draw\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import itertools\n",
    "import seaborn\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Data Exploration\n",
    "In this notebook, we will be examining the dataset by visualizing some examples and performing preliminary analyses. Note that the dataset used for training the models was not limited to APTOS 2019 Blindness Detection competition and used the previous Diabetic Retinopathy Detection competition as well.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Class statistics](#Class-statistics)\n",
    "2. [Visualizing the data](#Visualizing-the-data)\n",
    "3. [Image statistics](#Image-statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Class statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "dist = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "Let's begin by inspecting the labels in the APTOS dataset. These and all the other datasets contain the diagnosis information which is in the following integer units.\n",
    "\n",
    "0 - No DR\n",
    "\n",
    "1 - Mild\n",
    "\n",
    "2 - Moderate\n",
    "\n",
    "3 - Severe\n",
    "\n",
    "4 - Proliferative DR\n",
    "\n",
    "Note: DR = Diabetic Retinopathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['aptos'] = pd.read_csv(os.path.join( DATA_DIR['aptos'], 'train', 'labels.csv'))\n",
    "if plot:\n",
    "    display(labels['aptos'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the class distribution and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist['aptos'] = labels['aptos'].groupby(['diagnosis']).count()\n",
    "dist['aptos'].columns = ['count']\n",
    "if plot:\n",
    "    _ = dist['aptos'] .plot(kind = 'bar', width = 1, title='APTOS Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the photos contain `No DR`. To overcome the negative influences of this imbalance, we will need to employ measures later.\n",
    "Now we will inspect the class distributions for the external datasets. Note that we have access to the test labels for them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['chf'] = pd.concat((pd.read_csv(os.path.join( DATA_DIR['chf'], 'train', 'labels.csv')),\n",
    "                 pd.read_csv(os.path.join( DATA_DIR['chf'], 'test', 'labels.csv'))))\n",
    "if plot:\n",
    "    display(labels['chf'].head(10))\n",
    "dist['chf'] = labels['chf'].groupby(['diagnosis']).count()\n",
    "dist['chf'].columns = ['count']\n",
    "if plot:\n",
    "    _ = dist['chf'].plot(kind = 'bar', width = 1, title='CHF Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imbalance is even more pronounced in this larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDRiD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['idrid'] = pd.concat((pd.read_csv(os.path.join( DATA_DIR['idrid'], 'grading', 'train', \n",
    "                                           'labels.csv')),\n",
    "                             pd.read_csv(os.path.join( DATA_DIR['idrid'], 'grading', 'test', \n",
    "                                           'labels.csv'))))\n",
    "if plot:\n",
    "    display(labels['idrid'].head(10))\n",
    "dist['idrid'] = labels['idrid'].groupby(['diagnosis']).count()\n",
    "dist['idrid'].columns = ['count']\n",
    "if plot:\n",
    "    _ = dist['idrid'].plot(kind = 'bar', width = 1, title='IDRiD Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is much better but notice the lower counts. \n",
    "### Aggregate\n",
    "Let's now look at the aggregate distribution as we will be most likely using all of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist['agg'] = sum([dist[ds] for ds in ['aptos', 'chf', 'idrid']])\n",
    "if plot:\n",
    "    _=dist['agg'].plot(kind = 'bar', width = 1, title='Aggregate Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    _=plt.pie(dist['agg']['count'], labels=dist['agg'].index, autopct='%.1f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this is a highly imbalanced dataset. Also it is worth pointing out that with these external datasets we are expanding the size of our training data by about **24 folds**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((dist['agg'].sum() - dist['aptos'].sum())/ dist['aptos'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait. Are the images for these datasets comparable enough to aggregate them in the first place? Let's look and make a guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(paths, labels, imgs_to_show):\n",
    "    fig, axes = plt.subplots(int(imgs_to_show**.5), int(imgs_to_show**.5), figsize=(15, 15), facecolor='black')\n",
    "    for r in range(len(axes)):\n",
    "        for c in range(len(axes[0])):\n",
    "            path = paths[r * 4 + c]\n",
    "            image_id = path[path.find('img/')+4:-4]\n",
    "            diagnosis = labels[labels['id'] == image_id]['diagnosis']\n",
    "            axes[r, c].imshow(plt.imread(path))\n",
    "            axes[r, c].set_title('{}'.format(int(diagnosis)), color='w', style='oblique')\n",
    "            axes[r, c].axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {}\n",
    "imgs_to_show = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'aptos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[ds] = glob(os.path.join(DATA_DIR[ds], 'train', 'img', '*.png'))\n",
    "random.shuffle(paths[ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    plot_images(paths[ds], labels[ds], imgs_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the labels above the images are the diagnosis values. From inspecting a small subset of the APTOS images, we can see that there are several issues to pay attention to.\n",
    "\n",
    "* The ligthing conditions are not constant, which causes dramatic changes in average colors. \n",
    "* Some eyes are not fully in the image, they are clipped from top and bottom.\n",
    "* Left and right eyes are noticeable and different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'chf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[ds] = glob(os.path.join(DATA_DIR[ds], 'train', 'img', '*.jpeg'))\n",
    "random.shuffle(paths[ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    plot_images(paths[ds], labels[ds], imgs_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lighting othe CHF dataset appears to be less consistent. Same issues apply to here as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDRiD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'idrid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[ds] = glob(os.path.join(DATA_DIR[ds], 'grading', 'train', 'img', '*.jpg'))\n",
    "random.shuffle(paths[ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    plot_images(paths[ds], labels[ds], imgs_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, IDRiD dataset appears to have the most consistent data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Image statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful exploration is to actually quantify what we have done in the previous step. So, instead of guessing we will explore some quantifiable measures of the datasets.\n",
    "\n",
    "I find the following measures interesting for the challenge:\n",
    "\n",
    "* Mean/variance width and height of the image\n",
    "* Bounding region of the eye\n",
    "* Mean/variance of color of eye region (3 channel)\n",
    "* \\# of clipped vs full eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "template = {'width':[], 'height': [], 'blue': [], 'green': [], 'red': [], 'clipped_h':0, 'clipped_w':0}\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['figure.max_open_warning'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(mask):\n",
    "    h, w = mask.shape\n",
    "    floodfill = np.zeros((h+2, w+2), dtype=np.uint8)\n",
    "    floodfill[1:-1, 1:-1] = mask\n",
    "    floodfill_mask = np.zeros((h+4, w+4), dtype=np.uint8)\n",
    "    cv2.floodFill(floodfill, floodfill_mask, (0,0), 1)\n",
    "    floodfill_inv = cv2.bitwise_not(floodfill)[1:-1, 1:-1]\n",
    "    final_mask = mask | floodfill_inv\n",
    "    return final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats(path, return_img = False, return_mask = False):\n",
    "    stats = {'width':[], 'height': [], 'blue': [], 'green': [], 'red': [], 'clipped_h':0, 'clipped_w':0}\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    h, w, _ = img.shape\n",
    "    mask = find_eye(img)\n",
    "    eye_hh, eye_hw = np.where(mask)\n",
    "    eye_ww, eye_wh = np.where(mask.T)\n",
    "    clipped_h = eye_hh[0] == 0 or eye_hh[-1] == h-1\n",
    "    clipped_w = eye_ww[0] == 0 or eye_ww[-1] == w-1\n",
    "    blue, green, red = np.mean(img[np.where(mask)], axis=0)\n",
    "\n",
    "    stats['blue'].append(blue)\n",
    "    stats['green'].append(green)\n",
    "    stats['red'].append(red)\n",
    "    stats['height'].append(h)\n",
    "    stats['width'].append(w)\n",
    "    stats['clipped_h'] = clipped_h\n",
    "    stats['clipped_w'] = clipped_w\n",
    "    if return_mask:\n",
    "        if return_img:\n",
    "            return stats, img, mask\n",
    "        return stats, mask\n",
    "    if return_img:\n",
    "        return stats, img\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(dicts):\n",
    "    final_dict = {}\n",
    "    for key in template:\n",
    "        if type(template[key]) == list:\n",
    "            final_dict[key] = list(itertools.chain.from_iterable([d[key] for d in dicts]))\n",
    "        else:\n",
    "            final_dict[key] = sum([d[key] for d in dicts])\n",
    "    final_dict['N'] = len(dicts)\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(stats):\n",
    "    fig, axes = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n",
    "    axes[0].set_title('Height')\n",
    "    axes[0].set_xticks(np.arange(400, 3901, 500))\n",
    "    axes[0].hist(stats['height'], bins=np.arange(400, 3901, 250))\n",
    "    axes[1].set_title('Width')\n",
    "    axes[1].set_xticks(np.arange(400, 3901, 500))\n",
    "    _ = axes[1].hist(stats['width'], bins=np.arange(400, 3901, 250))\n",
    "    print('mean height: {:.2f}'.format(sum(stats['height'])/stats['N']))\n",
    "    print('mean width: {:.2f}'.format(sum(stats['width'])/stats['N']))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, sharey=True, figsize=(15, 5))\n",
    "    axes[0].set_title('Blue')\n",
    "    axes[0].set_xticks(np.arange(0, 256, 32))\n",
    "    axes[0].hist(stats['blue'], bins=np.arange(0, 256, 16))\n",
    "\n",
    "    axes[1].set_title('Green')\n",
    "    axes[1].set_xticks(np.arange(0, 256, 32))\n",
    "    axes[1].hist(stats['green'], bins=np.arange(0, 256, 16))\n",
    "\n",
    "    axes[2].set_title('Red')\n",
    "    axes[2].set_xticks(np.arange(0, 256, 32))\n",
    "    _ = axes[2].hist(stats['red'], bins=np.arange(0, 256, 16))\n",
    "\n",
    "    stats['mean_blue'] = mean_blue = sum(stats['blue'])/stats['N']\n",
    "    stats['std_blue'] = std_blue = np.std(stats['blue'])\n",
    "    print('mean blue: {:.2f}, std blue: {:.2f}'.format(mean_blue, std_blue))\n",
    "\n",
    "    stats['mean_green'] = mean_green = sum(stats['green'])/stats['N']\n",
    "    stats['std_green'] = std_green = np.std(stats['green'])\n",
    "    print('mean green: {:.2f}, std green: {:.2f}'.format(mean_green, std_green))\n",
    "\n",
    "    stats['mean_red'] = mean_red = sum(stats['red'])/stats['N']\n",
    "    stats['std_red'] = std_red = np.std(stats['red'])\n",
    "    print('mean red: {:.2f}, std red: {:.2f}'.format(mean_red, std_red))\n",
    "    px = np.round([mean_red, mean_green, mean_blue]).astype(np.uint8)[np.newaxis, np.newaxis, :]\n",
    "    im = np.zeros((10, 10, 3), dtype=np.uint8)\n",
    "    im = im + px \n",
    "    fig, axes = plt.subplots(1, 1, figsize=(15, 5))\n",
    "    fig.suptitle('Average color of the eye')\n",
    "    _=axes.imshow(im)\n",
    "    print('{:.2%} is clipped in height'.format(stats['clipped_h']/ stats['N']))\n",
    "    print('{:.2%} is clipped in width'.format(stats['clipped_w']/ stats['N']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aptos",
   "language": "python",
   "name": "aptos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
